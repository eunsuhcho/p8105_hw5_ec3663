---
title: "Homework 5"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)
library(purrr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Problem 1

For this problem, we are interested in data gathered and made public by _The Washington Post_ on homicides in 50 large U.S. cities. The code chunk below imports and cleans the data.

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>%
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) %>% 
  filter(city_state != "Tulsa, AL") 
```

The resulting dataframe has `r nrow(homicide_df)` entries, on variables that include the victim name, race, age, and sex; the date the homicide was reported; and the location of the homicide. In cleaning, I created a `city_state` variable that includes both city and state, and a `resolution` variable to indicate whether the case was closed by arrest. I also excluded one entry in Tulsa, AL, which is not a major US city and is most likely a data entry error. 

In the next code chunk, I group within cities and summarize to produce the total number of homicides and the number that are solved. 

```{r}
city_homicide_df = 
  homicide_df %>% 
  select(city_state, disposition, resolution) %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved"))
```

Focusing only on Baltimore, MD, I can use the `prop.test` and `broom::tidy` functions to obtain an estimate and CI of the proportion of unsolved homicides in that city. The table below shows those values.

```{r}
bmore_test = 
  prop.test(
    x = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_unsolved),
    n = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_total)) 

broom::tidy(bmore_test) %>% 
  knitr::kable(digits = 3)
```

Building on this code, I can use functions in the `purrr` package to obtain estimates and CIs for the proportion of unsolved homicides in each city in my dataset. The code below implements this analysis. 

```{r}
test_results = 
  city_homicide_df %>% 
  mutate(
    prop_tests = map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high) %>% 
  mutate(city_state = fct_reorder(city_state, estimate))
```

Finally, I make a plot showing the estimate (and CI) of the proportion of unsolved homicides in each city.

```{r}
test_results %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

This figure suggests a very wide range in the rate at which homicides are solved -- Chicago is noticeably high and, given the narrowness of the CI, likely is the location of many homicides. 

# Problem 2

```{r}
library(rvest)
```

Create a tidy dataframe

```{r}
names_df =
  tibble(
    files = list.files("data/"),
    path = str_c("data/", files)) |> 
  mutate(data = map(path, read_csv)) |> 
  unnest()

p2_df =
  names_df |> 
  mutate(
    files = str_replace(files, ".csv", ""),
    arm = str_sub(files, 1, 3),
    ID = str_sub(files, 5, 7)) |> 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "observation",
    names_prefix = "week_") |> 
  mutate(week = as.numeric(week)) |> 
  select(arm, ID, week, observation)
```

Create a spaghetti plot showing observations on each subject over time and comment

```{r}
p2_df |> 
  ggplot(aes(x = week, y = observation, color = ID)) +
  geom_line() +
  facet_wrap(~arm) +
  labs(x = "Week", y = "Observations", title = "Observations on Subjects over Time")
```

There was no marked increase or decrease in observation values for the participants in the control arm. The majority of the values in the control arm stayed between observation values -1.25 and 3.75. The values in the experimental arm, however, generally increased over the course of the 8 weeks. The values generally started at week 0 in the range of -1.25 to 3.75, but by week 8, the values fell within the range of 2.5 to 7.5.

# Problem 3

Set elements, generate datasets

```{r}
sim_mean = function(mu) {
  data = tibble(
    x = rnorm(n=30, mean = mu, sd = 5),
  )
output = data |>
  t.test() |>
  broom::tidy() |>
  select(estimate, p.value) |>
  rename(mu_hat=estimate, pval=p.value)
}
```

Repeat for mu = 1, 2, 3, 4, 5, 6
```{r}
sim_results = expand_grid(
  mu_df = c(0, 1, 2, 3, 4, 5, 6),
  iter = 1:5000
) |>
  mutate(
    estimate = map(mu_df, sim_mean)
  ) |>
  unnest(estimate)
```

Plot showing proportion of times null was rejected and true value of mu

```{r}
sim_results |>
  group_by(mu_df) |> 
  summarise(
    reject = sum(pval < 0.05),
    proport = reject/5000) |> 
  ggplot(aes(x = mu_df, y = proport)) +
  geom_point() +
  geom_line() +
  labs(x = "True value of μ", y = "Power", title = "Power of the test versus the true value of μ")
```

The plot depicts a curve of test power that increases between the true values of mu of 0 to around 4 until it plateaus. The plot indicates that as the effect size increases, the power of the test also increases.

Plot showing average estimate of mu_hat and true value of mu

```{r}
sim_results |> 
  group_by(mu_df) |> 
  mutate(mu_hat_avg = mean(mu_hat)) |> 
  ggplot(aes(x = mu_df, y = mu_hat_avg)) +
  geom_point() +
  geom_line() +
  labs(x = "True value of μ", y = "Average estimate of μ_hat", title = "Average estimate of μ_hat versus the true value of μ")
```

Plot showing average estimate of mu_hat for samples in which null was rejected and true value of mu

```{r}
sim_results |> 
  group_by(mu_df) |> 
  filter(pval<0.05) |> 
  mutate(mu_hat_avg = mean(mu_hat)) |> 
  ggplot(aes(x = mu_df, y = mu_hat_avg)) +
  geom_point() +
  geom_line() +
  labs(x = "True value of μ", y = "Average estimate of μ_hat", title = "Average estimate of μ_hat when null was rejected versus the true value of μ")
```

The sample average of mu_hat across tests for which the null is rejected is not approximately equal to the true value of mu. In the plot with the average mu_hat values when the null is rejected, we observe that the sample average of mu_hat is equal to the true value of mu for values 0, 4, 5, and 6. When the true value of mu is 1, 2, and 3, the average estimate of mu_hat is not equal to the true value. The average estimate of mu_hat may be unequal to the true value of mu, because a smaller effect size indicates a lower test power. This may influence the accuracy of the average estimate of mu_hat.
